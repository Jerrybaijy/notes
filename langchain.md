---
title: langchain
author: Jerry.Baijy
tags:
  - åº”ç”¨ç§‘å­¦
  - it
  - ai
  - ai-agent
  - framework
---

# LangChain

[LangChain](https://docs.langchain.com/oss/python/langchain/overview) æ˜¯ä¸€ä¸ªåº”ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºã€‚

> [LangChain æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/overview)
>
> [LangChain å‚è€ƒ](https://reference.langchain.com/python/langchain/)

LangChain çš„æ ¸å¿ƒä»·å€¼åœ¨äº**æ¨¡å—åŒ–**ï¼Œä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š

- **Models**ï¼šæä¾›ç»Ÿä¸€çš„æ¥å£ï¼Œè®©ä½ å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒçš„å¤§æ¨¡å‹ï¼ˆå¦‚OpenAIã€æœ¬åœ°æ¨¡å‹ï¼‰ã€‚
- **Prompts**ï¼šé«˜æ•ˆåœ°ç®¡ç†æç¤ºè¯æ¨¡æ¿ï¼Œä½¿äº¤äº’æ›´å¯æ§ã€‚
- **Indexes**ï¼šåŒ…å«æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨å’Œå‘é‡æ•°æ®åº“æ£€ç´¢å™¨ï¼Œä¸“é—¨ç”¨äºå¤„ç†å¤–éƒ¨æ•°æ®ã€‚
- **Chains**ï¼šå…è®¸ä½ å°†å¤šä¸ªæ­¥éª¤ï¼ˆå¦‚æç¤ºã€æ¨¡å‹è°ƒç”¨ã€è¾“å‡ºè§£æï¼‰è¿æ¥æˆä¸€ä¸ªå®Œæ•´çš„â€œå·¥ä½œé“¾â€ã€‚
- **Agents**ï¼šè®©å¤§æ¨¡å‹å…·å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨ã€æœç´¢å¼•æ“ï¼‰çš„èƒ½åŠ›ï¼Œä»¥å®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚
- **Memory**ï¼šç®¡ç†å¯¹è¯å†å²ï¼Œå®ç°æœ‰ä¸Šä¸‹æ–‡çš„å¤šè½®å¯¹è¯ã€‚

## åŸºæœ¬å®ç°

- åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º [Python è™šæ‹Ÿç¯å¢ƒ](python.md#è™šæ‹Ÿç¯å¢ƒ)å¹¶æ¿€æ´»ã€‚

- å®‰è£…å¿…è¦åº“

  ```bash
  pip install langchain
  pip install langchain-google-genai
  pip install python-dotenv
  ```

- `.env`

  ```
  # å°† YOUR_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çœŸå®çš„ Gemini API å¯†é’¥
  GEMINI_API_KEY="YOUR_API_KEY_HERE"
  ```

- `app.py`

  ```python
  from langchain.agents import create_agent
  import os
  from dotenv import load_dotenv
  from langchain_google_genai import ChatGoogleGenerativeAI
  
  # ---------------æŒ‡å®šæ¨¡å‹---------------------
  # è¯»å– .env æ–‡ä»¶å¹¶å°† GEMINI_API_KEY åŠ è½½åˆ°ç¯å¢ƒä¸­
  load_dotenv()
  
  # è¯»å– API å¯†é’¥
  GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
  
  # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å·²åŠ è½½
  if not GEMINI_API_KEY:
      raise ValueError("GEMINI_API_KEY æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ã€‚è¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶ã€‚")
  
  # å®ä¾‹åŒ–æ¨¡å‹
  model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY)
  
  # ----------------å®šä¹‰å·¥å…·-------------------------
  def get_weather(city: str) -> str:
      """Get weather for a given city."""
      return f"It's always sunny in {city}!"
  
  # ----------------åˆ›å»ºä»£ç†-------------------------
  agent = create_agent(
      model=model,
      tools=[get_weather],
      system_prompt="You are a helpful assistant",
  )
  
  # ----------------è°ƒç”¨ä»£ç†-------------------------
  result = agent.invoke(
      {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
  )
  
  print(result)
  ```

## æ ‡å‡†å®ç°

- åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º [Python è™šæ‹Ÿç¯å¢ƒ](python.md#è™šæ‹Ÿç¯å¢ƒ) å¹¶æ¿€æ´»ã€‚

- å®‰è£…å¿…è¦åº“

  ```bash
  pip install -U langchain
  pip install -U langchain-google-genai
  pip install python-dotenv
  ```

- `.env`

  ```
  # å°† YOUR_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çœŸå®çš„ Gemini API å¯†é’¥
  GEMINI_API_KEY="YOUR_API_KEY_HERE"
  ```

- `app.py`

  ```python
  from langchain.agents import create_agent
  import os
  from dotenv import load_dotenv
  from langchain_google_genai import ChatGoogleGenerativeAI
  from langchain.tools import tool, ToolRuntime
  from dataclasses import dataclass
  from langgraph.checkpoint.memory import InMemorySaver
  
  # ---------------å®šä¹‰ç³»ç»Ÿæç¤ºè¯­---------------------
  SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ç²¾é€šåŒå…³è¯­çš„ä¸“ä¸šå¤©æ°”é¢„æŠ¥å‘˜ï¼Œè¯·å§‹ç»ˆä½¿ç”¨**ä¸­æ–‡**å›ç­”ã€‚
  
  ä½ æ‹¥æœ‰ä»¥ä¸‹ä¸¤ä¸ªå·¥å…·ï¼š
  
  - get_weather_for_location: ç”¨äºè·å–ç‰¹å®šåœ°ç‚¹çš„å¤©æ°”
  - get_user_location: ç”¨äºè·å–ç”¨æˆ·å½“å‰æ‰€åœ¨åœ°ç‚¹
  
  å¦‚æœç”¨æˆ·è¯¢é—®å¤©æ°”ï¼Œè¯·ç¡®ä¿ä½ çŸ¥é“åœ°ç‚¹ã€‚å¦‚æœç”¨æˆ·çš„é—®é¢˜æš—ç¤ºçš„æ˜¯ä»–ä»¬å½“å‰æ‰€åœ¨çš„ä½ç½®ï¼Œåˆ™ä½¿ç”¨ get_user_location å·¥å…·æ¥æŸ¥æ‰¾ä»–ä»¬çš„ä½ç½®ã€‚
  **è¯·æ³¨æ„ï¼šä½ çš„æ‰€æœ‰å›å¤éƒ½å¿…é¡»æ˜¯ä¸­æ–‡çš„ï¼Œå¹¶ä¸”åº”åŒ…å«ä¸­æ–‡çš„åŒå…³è¯­æˆ–æœ‰è¶£çš„è°éŸ³æ¢—ã€‚**"""
  
  # ---------------æŒ‡å®šæ¨¡å‹---------------------
  # è¯»å– .env æ–‡ä»¶å¹¶å°† GEMINI_API_KEY åŠ è½½åˆ°ç¯å¢ƒä¸­
  load_dotenv()
  
  # è¯»å– API å¯†é’¥
  GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
  
  # æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å·²åŠ è½½
  if not GEMINI_API_KEY:
      raise ValueError("GEMINI_API_KEY æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ã€‚è¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶ã€‚")
  
  # å®ä¾‹åŒ–æ¨¡å‹
  llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY)
  
  # ---------------å®šä¹‰ä¸Šä¸‹æ–‡æ¨¡å¼ context_schema---------------------
  @dataclass
  class Context:
      """Custom runtime context schema."""
  
      user_id: str
  
  # ---------------å®šä¹‰å“åº”æ ¼å¼ response_format---------------------
  @dataclass
  class ResponseFormat:
      """Response schema for the agent."""
  
      punny_response: str
      weather_conditions: str | None = None
  
  # ---------------å®šä¹‰å†…å­˜å­˜å‚¨å™¨---------------------
  checkpointer = InMemorySaver()
  
  # ä¼šè¯æ ‡è¯†ç¬¦
  config = {"configurable": {"thread_id": "1"}}
  
  # ----------------å®šä¹‰å·¥å…·-------------------------
  @tool
  def get_weather_for_location(city: str) -> str:
      """Get weather for a given city."""
      return f"It's always sunny in {city}!"
  
  @tool
  def get_user_location(runtime: ToolRuntime[Context]) -> str:
      """Retrieve user information based on user ID."""
      user_id = runtime.context.user_id
      return "Florida" if user_id == "1" else "SF"
  
  # ----------------åˆ›å»ºä»£ç†-------------------------
  agent = create_agent(
      model=llm,
      system_prompt=SYSTEM_PROMPT,
      tools=[get_user_location, get_weather_for_location],
      context_schema=Context,
      response_format=ResponseFormat,
      checkpointer=checkpointer,
  )
  
  # ----------------è¿è¡Œä»£ç†-------------------------
  
  # è¿è¡Œ Agentï¼Œä¼ å…¥ç”¨æˆ·æ¶ˆæ¯å’Œé…ç½®
  response = agent.invoke(
      {"messages": [{"role": "user", "content": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]},
      config=config,  # ä¼ å…¥ä¼šè¯ IDï¼Œè®© Agent çŸ¥é“å»å“ªé‡ŒåŠ è½½/å­˜å‚¨è®°å¿†ã€‚
      context=Context(user_id="1"),  # ä¼ å…¥å·¥å…·æ‰€éœ€çš„è¿è¡Œæ—¶ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆç”¨æˆ·IDä¸º "1"ï¼‰ã€‚
  )
  
  # æ‰“å° Agent æœ€ç»ˆçš„ç»“æ„åŒ–å›å¤
  print(response["structured_response"])
  # é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼šResponseFormat(punny_response="...", weather_conditions="...")
  
  # å†æ¬¡è¿è¡Œ Agentï¼Œä½¿ç”¨ç›¸åŒçš„ `thread_id`ï¼ŒAgent ä¼šè‡ªåŠ¨åŠ è½½ç¬¬ä¸€æ¬¡è°ƒç”¨çš„å†å²è®°å½•ã€‚
  response = agent.invoke(
      {"messages": [{"role": "user", "content": "è°¢è°¢ï¼"}]},
      config=config,  # ç›¸åŒ configï¼Œç»§ç»­ä¼šè¯ ID "1" çš„å†å²ã€‚
      context=Context(user_id="1"),  # ä»ç„¶ä¼ å…¥ä¸Šä¸‹æ–‡æ•°æ®ã€‚
  )
  
  # æ‰“å° Agent æœ€ç»ˆçš„ç»“æ„åŒ–å›å¤
  print(response["structured_response"])
  # é¢„æœŸè¾“å‡ºç¤ºä¾‹ï¼šResponseFormat(punny_response="You're 'thund-erfully' welcome!...", weather_conditions=None)
  ```

# Models

[Models](https://docs.langchain.com/oss/python/langchain/models) æ˜¯ Agent çš„æ¨ç†å¼•æ“ã€‚å®ƒä»¬é©±åŠ¨æ™ºèƒ½ä½“çš„å†³ç­–è¿‡ç¨‹ã€‚LangChain æä¾›äº†[ä¸Šåƒç§æ¨¡å‹é›†æˆåŒ…](https://docs.langchain.com/oss/python/integrations/providers/all_providers)ã€‚

æ¨¡å‹å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼åŠ ä»¥åˆ©ç”¨ï¼š

- **ç‹¬ç«‹è¿è¡Œ**ï¼šå¯ä»¥ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼ˆåœ¨ä»£ç†å¾ªç¯ä¹‹å¤–ï¼‰æ¥æ‰§è¡Œæ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»æˆ–æå–ç­‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»£ç†æ¡†æ¶ã€‚
- **ä½¿ç”¨ä»£ç†**ï¼šé€šè¿‡ Agent å¼•å…¥ã€‚

## åŸºæœ¬å®ç°

ä¸‹é¢æ˜¯ä¸€ä¸ªå®ç°å¤šè½®å¾ªç¯é—®ç­”çš„æ¨¡å‹ï¼ˆæ— è®°å¿†ï¼‰

```
# å°† YOUR_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çœŸå®çš„ Gemini API å¯†é’¥
GEMINI_API_KEY="YOUR_API_KEY_HERE"
```

```bash
pip install python-dotenv
pip install langchain
pip install langchain-google-genai
```

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_agent

# è®¾ç½® API
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# å®ä¾‹åŒ–æ¨¡å‹
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GEMINI_API_KEY
)

# è°ƒç”¨æ¨¡å‹ & å¤šè½®é—®ç­”
while True:
    question = input("ç”¨æˆ·ï¼š")
    response = model.invoke(question)
    print("AIï¼š" + response.content)
    print("=" * 60)
```

## è®¾ç½® API

LLM çš„ APIï¼Œæ­¤å¤„ä½¿ç”¨[å…è´¹çš„ Google AI Studio çš„ API](https://aistudio.google.com/api-keys)ï¼Œæœ‰ä¸¤ç§è®¾ç½®æ–¹æ³•ï¼š

- åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®
- åœ¨æœ¬åœ°å½“å‰ç¯å¢ƒçš„ç»ˆç«¯ä¸­è®¾ç½®

### `.env`

```
# å°† YOUR_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çœŸå®çš„ Gemini API å¯†é’¥
GEMINI_API_KEY="YOUR_API_KEY_HERE"
```

```bash
pip install python-dotenv
```

```python
import os
from dotenv import load_dotenv

# è¯»å– .env æ–‡ä»¶å¹¶å°† GEMINI_API_KEY åŠ è½½åˆ°ç¯å¢ƒä¸­
load_dotenv()

# è¯»å– API å¯†é’¥
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# æ£€æŸ¥ API å¯†é’¥æ˜¯å¦å·²åŠ è½½ï¼Œå¯é€‰ã€‚
if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY æœªåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ã€‚è¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶ã€‚")
```

**åœ¨ä»¥ä¸Šä»£ç ä¸­ï¼š**

- `GEMINI_API_KEY` ä¸ºä» `.env` æ–‡ä»¶è¯»å–åˆ°çš„ APIã€‚
- ä¸è¦ä½¿ç”¨ `gemini-2.5-pro` æ¨¡å‹ï¼Œä»¥é˜²é…é¢è¶…é™ã€‚

### ç»ˆç«¯åŠ è½½

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export GOOGLE_API_KEY="YOUR_API_KEY_HERE"

# æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼Œå¦‚æœè¿”å› APIï¼Œåˆ™ä»£è¡¨è®¾ç½®æˆåŠŸã€‚
echo $GOOGLE_API_KEY
```

## æŒ‡å®šæ¨¡å‹

æŒ‡å®š Model æœ‰ä¸¤ç§æ–¹æ³•ï¼š

- åˆå§‹åŒ–æ¨¡å‹
- æ¨¡å‹ç±»

### åˆå§‹åŒ–æ¨¡å‹

[**åˆå§‹åŒ–æ¨¡å‹**](https://docs.langchain.com/oss/python/langchain/models#initialize-a-model)ï¼Œå³ä½¿ç”¨ [`init_chat_model`](https://reference.langchain.com/python/langchain/models/?_gl=1*1gysoxw*_gcl_au*NDczNTIxMDkyLjE3NjEzODI3OTI.*_ga*MjE5MjYzNzI5LjE3NjE1NTY1MzU.*_ga_47WX3HKKY2*czE3NjIxMDUzMjYkbzMwJGcxJHQxNzYyMTA1NzA1JGo1NSRsMCRoMA..#langchain.chat_models.init_chat_model) åˆå§‹åŒ–ä¸€ä¸ª Modelã€‚é€šå¸¸ç”¨äº Model çš„ç‹¬ç«‹è¿è¡Œï¼Œè€Œä¸æ˜¯å€ŸåŠ© Agentã€‚

```bash
pip install langchain
pip install langchain-google-genai
```

```python
from langchain.chat_models import init_chat_model

# ======================
# è¿˜åº”è¯¥è®¾å®š API å¯†é’¥ç­‰ç¯å¢ƒå˜é‡
# ======================

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    model="gemini-2.5-flash",
    api_key=GEMINI_API_KEY,
)

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ä½ æ˜¯è°ï¼Ÿ")
print(response.content)
```

### æ¨¡å‹ç±»

é€šè¿‡ `ChatGoogleGenerativeAI()` ç±»å®ä¾‹åŒ–ä¸€ä¸ª Modelã€‚

```bash
pip install langchain
pip install langchain-google-genai
```

```python
from langchain.agents import create_agent
from langchain_google_genai import ChatGoogleGenerativeAI

# ======================
# è¿˜åº”è¯¥è®¾å®š API å¯†é’¥ç­‰ç¯å¢ƒå˜é‡
# ======================

# å®ä¾‹åŒ–æ¨¡å‹
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GEMINI_API_KEY
)

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ä½ æ˜¯è°ï¼Ÿ")
print(response.content) # æˆ‘æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”± Google è®­ç»ƒã€‚
```

### æœ¬åœ°æ¨¡å‹

[Ollama](https://docs.langchain.com/oss/python/integrations/chat/ollama) å…è®¸æ‚¨åœ¨æœ¬åœ°è¿è¡Œå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## è°ƒç”¨æ¨¡å‹

æœ‰ä¸‰ç§ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•éƒ½é€‚ç”¨äºä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚

- `invoke`
- `stream`
- `batch`

### `invoke`

[`invoke`](https://docs.langchain.com/oss/python/langchain/models#invoke) æ˜¯æœ€ç›´æ¥çš„è°ƒç”¨æ–¹æ³•ã€‚

```python
response = model.invoke("Why do parrots have colorful feathers?")
print(response)
```

### `stream`

[`stream`](https://docs.langchain.com/oss/python/langchain/models#stream) å¯ä»¥é€æ­¥æ˜¾ç¤ºè¾“å‡ºã€‚

```python
for chunk in model.stream("Why do parrots have colorful feathers?"):
    print(chunk.text, end="|", flush=True)
```

### `batch`

[`batch`](https://docs.langchain.com/oss/python/langchain/models#batch) å°†ä¸€ç³»åˆ—ç‹¬ç«‹çš„æ¨¡å‹è¯·æ±‚æ‰¹é‡å¤„ç†ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½å¹¶é™ä½æˆæœ¬ï¼Œå› ä¸ºå¯ä»¥å¹¶è¡Œå¤„ç†è¿™äº›è¯·æ±‚ï¼š

```python
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
```

## å¤šè½®é—®ç­”

```python
# ======================
# è®¾å®š APIï¼ŒæŒ‡å®šæ¨¡å‹
# ======================

# å¤šè½®é—®ç­”
while True:
    question = input("ç”¨æˆ·ï¼š")
    response = model.invoke(question)
    print("AIï¼š" + response.content)
    print("=" * 60)
```

## å¤šæ¨¡æ€

[å¤šæ¨¡æ€](https://docs.langchain.com/oss/python/langchain/models#multimodal)ï¼ŒæŸäº›æ¨¡å‹å¯ä»¥å¤„ç†å¹¶è¿”å›**éæ–‡æœ¬æ•°æ®**ï¼Œä¾‹å¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚æ‚¨å¯ä»¥é€šè¿‡æä¾›å†…å®¹å—å°†éæ–‡æœ¬æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚

```python
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)
# [
#     {"type": "text", "text": "Here's a picture of a cat"},
#     {"type": "image", "base64": "...", "mime_type": "image/jpeg"},
# ]
```

## æ¨¡å‹å…¶å®ƒ

- [è°ƒç”¨ Tools](https://docs.langchain.com/oss/python/langchain/models#tool-calling)
- [ç»“æ„åŒ–è¾“å‡º](https://docs.langchain.com/oss/python/langchain/models#structured-outputs)
- æ›´å¤š...

# Agents

[Agents](https://docs.langchain.com/oss/python/langchain/agents) ç»„ä»¶ç”¨äºåˆ›å»º Agentã€‚è®©å¤§æ¨¡å‹å…·å¤‡ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚è®¡ç®—å™¨ã€æœç´¢å¼•æ“ï¼‰çš„èƒ½åŠ›ï¼Œä»¥å®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚

```mermaid
flowchart TD
    %% èŠ‚ç‚¹
    input([input])
    model{model}
    tools(tools)
    output([output])

    %% ç»“æ„
    input --> model
    model -- action --> tools
    tools -- observation --> model
    model -- finish --> output
```

## åŸºæœ¬å®ç°

ä¸‹é¢æ˜¯ä¸€ä¸ªå®ç°å¤šè½®å¾ªç¯é—®ç­”çš„ä»£ç†ï¼ˆæœ‰è®°å¿†ï¼‰

```
# å°† YOUR_API_KEY_HERE æ›¿æ¢ä¸ºæ‚¨çœŸå®çš„ Gemini API å¯†é’¥
GEMINI_API_KEY="YOUR_API_KEY_HERE"
```

```
pip install python-dotenv
pip install langchain
pip install langchain-google-genai
```

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

# ---------------æŒ‡å®šæ¨¡å‹---------------------
# è®¾ç½® API
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# å®ä¾‹åŒ–æ¨¡å‹
model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", api_key=GEMINI_API_KEY)

# ----------------åˆ›å»ºä»£ç†-------------------------
agent = create_agent(
    model=model,
    checkpointer=InMemorySaver() # å®ç°è®°å¿†
)

# ----------------è°ƒç”¨ä»£ç† & å¤šè½®é—®ç­”----------------
while True:
    question = input("ç”¨æˆ·ï¼š")
    response = agent.invoke(
        {"messages": [{"role": "user", "content": question}]},
        {"configurable": {"thread_id": "1"}}, # å®ç°è®°å¿†
    )
    print("AIï¼š" + response["messages"][-1].content)
    print("=" * 60)
```

**åœ¨ä»¥ä¸Šä»£ç ä¸­ï¼š**

- [`create_agent()`](https://reference.langchain.com/python/langchain/agents/?_gl=1*z2ejvz*_gcl_au*NDczNTIxMDkyLjE3NjEzODI3OTI.*_ga*MjE5MjYzNzI5LjE3NjE1NTY1MzU.*_ga_47WX3HKKY2*czE3NjIwODQyMDAkbzI4JGcxJHQxNzYyMDg0MzczJGo2MCRsMCRoMA..#langchain.agents.create_agent)ï¼šç”¨äºåˆ›å»º Agentã€‚
- `model=model` ï¼š`model` ä¸ºä¹‹å‰åˆ›å»ºçš„å®ä¾‹åŒ–æ¨¡å‹ã€‚

## é™æ€å’ŒåŠ¨æ€æ¨¡å‹

### é™æ€æ¨¡å‹

[**é™æ€æ¨¡å‹**](https://docs.langchain.com/oss/python/langchain/agents#static-model)åœ¨åˆ›å»ºä»£ç†æ—¶é…ç½®ä¸€æ¬¡ï¼Œå¹¶åœ¨æ•´ä¸ªæ‰§è¡Œè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚è¯¦è§ Agent çš„åŸºæœ¬å®ç°ã€‚

### åŠ¨æ€æ¨¡å‹

[**åŠ¨æ€æ¨¡å‹**](https://docs.langchain.com/oss/python/langchain/agents#dynamic-model)æ˜¯åŸºäºå½“å‰çŠ¶æ€ä»¥åŠä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€‰æ‹©ä¸åŒçš„æ¨¡å‹ã€‚è¿™ä½¿å¾—å¤æ‚çš„è·¯ç”±é€»è¾‘å’Œæˆæœ¬ä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚

è¦ä½¿ç”¨åŠ¨æ€æ¨¡å‹ï¼Œä½¿ç”¨ [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) è£…é¥°å™¨åˆ›å»ºä¸­é—´ä»¶ï¼Œè¯¥è£…é¥°å™¨ä¼šåœ¨è¯·æ±‚ä¸­ä¿®æ”¹æ¨¡å‹ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse

basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """Choose model based on conversation complexity."""
    message_count = len(request.state["messages"])

    if message_count > 10:
        # Use an advanced model for longer conversations
        model = advanced_model
    else:
        model = basic_model

    request.model = model
    return handler(request)

agent = create_agent(
    model=basic_model,  # Default model
    tools=tools,
    middleware=[dynamic_model_selection]
)
```

## `context_schema`

`context_schema` ä¸Šä¸‹æ–‡æ¨¡å¼ï¼Œç”¨äºå®šä¹‰ç”¨æˆ·çš„**å…ƒæ•°æ®**ï¼ŒAgent åœ¨è¿è¡Œæ—¶å¯ä»¥è¯†åˆ«ç”¨æˆ·ã€‚

```python
from dataclasses import dataclass

# å®šä¹‰ schema
@dataclass
class Context:
    """Custom runtime context schema.""" 

    user_id: str

# åœ¨å®šä¹‰å·¥å…·æ—¶ï¼Œä¼ å…¥ schema
@tool
def get_user_location(runtime: ToolRuntime[Context]) -> str:
    """Retrieve user information based on user ID."""
    user_id = runtime.context.user_id
    return "Florida" if user_id == "1" else "SF"

# ä¼ å…¥ Agent
agent = create_agent(
    # ... å…¶ä»–å‚æ•°
    context_schema=Context
)

# åœ¨å›åº”æ—¶ï¼Œä¼ å…¥ schema
response = agent.invoke(
    # ... å…¶å®ƒæ¶ˆæ¯å’Œé…ç½®
    
    context=Context(user_id="1")
)
```

## `response_format`

`response_format` å“åº”æ ¼å¼ï¼Œ

```python
# å®šä¹‰å“åº”æ ¼å¼
@dataclass
class ResponseFormat:
    """Response schema for the agent."""

    punny_response: str
    weather_conditions: str | None = None

# ä¼ å…¥ Agent
agent = create_agent(
    # ... å…¶ä»–å‚æ•°
    response_format=ResponseFormat
)
```

# Memory

**è®°å¿†ç³»ç»Ÿ**èƒ½å¤Ÿè®°ä½ä»¥å¾€äº¤äº’çš„ä¿¡æ¯ã€‚

## Short-term Memory

[**Short-term Memory**](https://docs.langchain.com/oss/python/langchain/short-term-memory)ï¼ˆçŸ­æœŸè®°å¿†ï¼‰ï¼šå…³æ³¨äº**å•ä¸ªå¯¹è¯ä¼šè¯**ï¼ˆæˆ–â€œçº¿ç¨‹â€ï¼‰ä¸­çš„**å³æ—¶ä¸Šä¸‹æ–‡**ã€‚å®ƒçš„ä¸»è¦ç›®æ ‡æ˜¯è®© LLM èƒ½å¤Ÿè®°ä½æœ€è¿‘çš„äº’åŠ¨ï¼Œä»è€Œå®ç°æµç•…ã€å¤šè½®æ¬¡çš„å¯¹è¯ã€‚

### åŸºæœ¬ç”¨æ³•

åœ¨åˆ›å»º Agent æ—¶æŒ‡å®š [`checkpointer`](https://reference.langchain.com/python/langchain/agents/?h=checkpointer#langchain.agents.create_agent(checkpointer)) å±æ€§ï¼Œåœ¨è°ƒç”¨ Agent æ—¶ä½¿ç”¨ `thread_id`ã€‚

æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œå®ƒåªåœ¨å½“å‰ç¨‹åºè¿è¡ŒæœŸé—´æœ‰æ•ˆã€‚ä¸€æ—¦ç¨‹åºç»“æŸæˆ–é‡å¯ï¼Œè¿™äº›è®°å¿†å°±ä¼šä¸¢å¤±ã€‚

```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

# ======================
# è®¾å®š APIï¼ŒæŒ‡å®šæ¨¡å‹
# ======================

# ----------------åˆ›å»ºä»£ç†-------------------------
agent = create_agent(
    model=model,
    # æŒ‡å®š checkpointer å‚æ•°
    checkpointer=InMemorySaver()
)

# è°ƒç”¨ä»£ç† & å¤šè½®é—®ç­”
while True:
    question = input("ç”¨æˆ·ï¼š")
    response = agent.invoke(
        {"messages": [{"role": "user", "content": question}]},
        # æŒ‡å®šçº¿ç¨‹
        {"configurable": {"thread_id": "1"}},
    )
    print("AIï¼š" + response["messages"][-1].content)
    print("=" * 60)
```

**åœ¨ä»¥ä¸Šç¤ºä¾‹ä¸­ï¼š**

- `checkpointer=InMemorySaver()`ï¼š`InMemorySaver` å……å½“äº† Agent çš„**è®°å¿†å­˜å‚¨ä»‹è´¨**ã€‚å®ƒå°† Agent çš„æ•´ä¸ªæ‰§è¡Œå†å²ä¿å­˜åœ¨**å†…å­˜**ä¸­ã€‚
- `thread_id`ï¼šç”¨äºåŒºåˆ†ä¼šè¯ï¼Œå‘Šè¯‰ `InMemorySaver` å°†å½“å‰è¿™æ¬¡ `invoke` è°ƒç”¨çš„çŠ¶æ€å’Œå†å²ï¼Œä¿å­˜åˆ° ID ä¸º "1" çš„è¿™ä¸ªç‰¹å®šä¼šè¯ï¼ˆæˆ–çº¿ç¨‹ï¼‰ä¸­ã€‚

### ç”Ÿäº§ç”¨æ³•

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨ç”±æ•°æ®åº“æ”¯æŒçš„æ£€æŸ¥ç‚¹ã€‚

æ•°æ®å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œå³ä½¿ç¨‹åºå´©æºƒã€é‡å¯æˆ–éƒ¨ç½²åˆ°ä¸åŒçš„ç¯å¢ƒä¸­ï¼Œè®°å¿†ä¹Ÿä¸ä¼šä¸¢å¤±ã€‚

```bash
pip install langgraph-checkpoint-postgres
```

```python
from langchain.agents import create_agent
from langgraph.checkpoint.postgres import PostgresSaver  

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
    checkpointer.setup() # auto create tables in PostgresSql
    agent = create_agent(
        "gpt-5",
        [get_user_info],
        checkpointer=checkpointer,  
    )
```

**åœ¨ä»¥ä¸Šç¤ºä¾‹ä¸­ï¼š**

- `with è¯­å¥`ï¼šå»ºç«‹ä¸æ•°æ®åº“çš„è¿æ¥

### é˜²æ­¢è¶…é™

é•¿æ—¶é—´çš„å¯¹è¯å¯èƒ½ä¼šè¶…å‡º LLM çš„ä¸Šä¸‹æ–‡çª—å£ã€‚å¸¸è§çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ï¼š

- [ä¿®å‰ªæ¶ˆæ¯](https://docs.langchain.com/oss/python/langchain/short-term-memory#trim-messages)
- [åˆ é™¤æ¶ˆæ¯](https://docs.langchain.com/oss/python/langchain/short-term-memory#delete-messages)
- [æ¶ˆæ¯æ‘˜è¦](https://docs.langchain.com/oss/python/langchain/short-term-memory#summarize-messages)

### è®¿é—®å­˜å‚¨å™¨

[**è®¿é—®å­˜å‚¨å™¨**](https://docs.langchain.com/oss/python/langchain/short-term-memory#access-memory)ï¼šå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è®¿é—®å’Œä¿®æ”¹æ™ºèƒ½ä½“çš„çŸ­æœŸè®°å¿†ï¼ˆçŠ¶æ€ï¼‰ã€‚

## Long-term Memory

[**Long-term Memory**](https://docs.langchain.com/oss/python/langchain/long-term-memory)ï¼ˆé•¿æœŸè®°å¿†ï¼‰ï¼šå…³æ³¨**è·¨ä¼šè¯**å’Œ**è·¨æ—¶é—´**ä¿ç•™ä¿¡æ¯ï¼Œè®©ç³»ç»Ÿèƒ½å¤Ÿå­¦ä¹ ç”¨æˆ·åå¥½ã€ç»´æŠ¤ç”¨æˆ·æ¡£æ¡ˆæˆ–è®¿é—®å¹¿æ³›çš„é¢†åŸŸçŸ¥è¯†ã€‚

LangChain ä½¿ç”¨ [LangGraph æŒä¹…åŒ–](https://docs.langchain.com/oss/python/langgraph/persistence#memory-store)æ¥å®ç°é•¿æœŸè®°å¿†ã€‚

# Messages

[**Messages**](https://docs.langchain.com/oss/python/langchain/messages) æ˜¯æ¨¡å‹çš„åŸºæœ¬ä¸Šä¸‹æ–‡å•å…ƒã€‚å®ƒä»¬ä»£è¡¨æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºï¼Œæºå¸¦ä¸ LLM äº¤äº’æ—¶è¡¨ç¤ºå¯¹è¯çŠ¶æ€æ‰€éœ€çš„å†…å®¹å’Œå…ƒæ•°æ®ã€‚

## åŸºæœ¬ç”¨æ³•

### æ–‡æœ¬æç¤º

[**æ–‡æœ¬æç¤º**](https://docs.langchain.com/oss/python/langchain/messages#text-prompts)æ˜¯å­—ç¬¦ä¸²â€”â€”éå¸¸é€‚åˆç®€å•çš„ç”Ÿæˆä»»åŠ¡ï¼Œæ— éœ€ä¿ç•™å¯¹è¯å†å²è®°å½•ã€‚

```python
response = model.invoke("Write a haiku about spring")
```

### æ¶ˆæ¯æç¤º

[**æ¶ˆæ¯æç¤º**](https://docs.langchain.com/oss/python/langchain/messages#message-prompts)é€šè¿‡æä¾›æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼Œå°†æ¶ˆæ¯åˆ—è¡¨ä¼ é€’ç»™æ¨¡å‹ã€‚

```python
from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage("You are a poetry expert"),
    HumanMessage("Write a haiku about spring"),
    AIMessage("Cherry blossoms bloom...")
]
response = model.invoke(messages)
```

### å­—å…¸æ ¼å¼

[**å­—å…¸æ ¼å¼**](https://docs.langchain.com/oss/python/langchain/messages#dictionary-format)ä»¥è‡ªåŠ¨è¡¥å…¨æ ¼å¼æŒ‡å®šæ¶ˆæ¯ã€‚

```python
messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]
response = model.invoke(messages)
```

## æ¶ˆæ¯ç±»å‹

[**æ¶ˆæ¯ç±»å‹**](https://docs.langchain.com/oss/python/langchain/messages#message-types)ï¼š

-  [SystemMessage()](https://docs.langchain.com/oss/python/langchain/messages#system-message)ï¼šç³»ç»Ÿæ¶ˆæ¯ï¼Œå‘Šè¯‰æ¨¡å‹å¦‚ä½•è¿è¡Œï¼Œå¹¶ä¸ºäº¤äº’æä¾›ä¸Šä¸‹æ–‡ã€‚
-  [HumanMessage()](https://docs.langchain.com/oss/python/langchain/messages#human-message)ï¼šäººç±»æ¶ˆæ¯ï¼Œä»£è¡¨ç”¨æˆ·è¾“å…¥ä»¥åŠä¸æ¨¡å‹çš„äº¤äº’ã€‚
-  [AIMessage()](https://docs.langchain.com/oss/python/langchain/messages#ai-message)ï¼šAI æ¶ˆæ¯ï¼Œæ¨¡å‹ç”Ÿæˆçš„å“åº”ï¼ŒåŒ…æ‹¬æ–‡æœ¬å†…å®¹ã€å·¥å…·è°ƒç”¨å’Œå…ƒæ•°æ®ã€‚
-  [ToolMessage()](https://docs.langchain.com/oss/python/langchain/messages#tool-message)ï¼šå·¥å…·æ¶ˆæ¯ï¼Œå·¥å…·è°ƒç”¨çš„è¾“å‡ºã€‚

# Middleware

[**Middleware**](https://docs.langchain.com/oss/python/langchain/middleware)ï¼Œä¸­é—´ä»¶ï¼Œæä¾›äº†ä¸€ç§æ›´ä¸¥æ ¼åœ°æ§åˆ¶ä»£ç†å†…éƒ¨è¿è¡Œæ–¹å¼çš„æ–¹æ³•ã€‚

> [ä¸­é—´ä»¶æŒ‡å—](https://docs.langchain.com/oss/python/langchain/middleware)
>
> [ä¸­é—´ä»¶ API å‚è€ƒ](https://reference.langchain.com/python/langchain/middleware/?_gl=1*7nl9a8*_gcl_au*NDczNTIxMDkyLjE3NjEzODI3OTI.*_ga*MjE5MjYzNzI5LjE3NjE1NTY1MzU.*_ga_47WX3HKKY2*czE3NjIxNjg1MDEkbzM2JGcxJHQxNzYyMTcwOTc5JGo0JGwwJGgw)

ä¸­é—´ä»¶æœ‰ä»¥ä¸‹ç±»å‹ï¼š

- [å†…ç½®ä¸­é—´ä»¶](https://docs.langchain.com/oss/python/langchain/middleware#built-in-middleware)
- [è‡ªå®šä¹‰ä¸­é—´ä»¶](https://docs.langchain.com/oss/python/langchain/middleware#custom-middleware)
  - [åŸºäºè£…é¥°å™¨çš„ä¸­é—´ä»¶](https://docs.langchain.com/oss/python/langchain/middleware#decorator-based-middleware)
  - [åŸºäºç±»çš„ä¸­é—´ä»¶](https://docs.langchain.com/oss/python/langchain/middleware#class-based-middleware)

## åŸºæœ¬å®ç°

```python
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[SummarizationMiddleware(), HumanInTheLoopMiddleware()],
)
```

**åœ¨ä»¥ä¸Šç¤ºä¾‹ä¸­ï¼š**

- [`middleware`](https://reference.langchain.com/python/langchain/agents/?_gl=1*z2ejvz*_gcl_au*NDczNTIxMDkyLjE3NjEzODI3OTI.*_ga*MjE5MjYzNzI5LjE3NjE1NTY1MzU.*_ga_47WX3HKKY2*czE3NjIwODQyMDAkbzI4JGcxJHQxNzYyMDg0MzczJGo2MCRsMCRoMA..#langchain.agents.create_agent(middleware))ï¼šä¸­é—´ä»¶

## åŸºäºè£…é¥°å™¨çš„ä¸­é—´ä»¶

[**åŸºäºè£…é¥°å™¨çš„ä¸­é—´ä»¶**](https://docs.langchain.com/oss/python/langchain/middleware#decorator-based-middleware)æ˜¯ä¸€ç§è‡ªå®šä¹‰ä¸­é—´ä»¶ã€‚

```python
from langchain.agents.middleware import before_model, after_model, wrap_model_call
from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse, dynamic_prompt
from langchain.messages import AIMessage
from langchain.agents import create_agent
from langgraph.runtime import Runtime
from typing import Any, Callable

# Node-style: logging before model calls
@before_model
def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    print(f"About to call model with {len(state['messages'])} messages")
    return None

# Node-style: validation after model calls
@after_model(can_jump_to=["end"])
def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    last_message = state["messages"][-1]
    if "BLOCKED" in last_message.content:
        return {
            "messages": [AIMessage("I cannot respond to that request.")],
            "jump_to": "end"
        }
    return None

# Wrap-style: retry logic
@wrap_model_call
def retry_model(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    for attempt in range(3):
        try:
            return handler(request)
        except Exception as e:
            if attempt == 2:
                raise
            print(f"Retry {attempt + 1}/3 after error: {e}")

# Wrap-style: dynamic prompts
@dynamic_prompt
def personalized_prompt(request: ModelRequest) -> str:
    user_id = request.runtime.context.get("user_id", "guest")
    return f"You are a helpful assistant for user {user_id}. Be concise and friendly."

# Use decorators in agent
agent = create_agent(
    model="gpt-4o",
    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],
    tools=[...],
)
```

## åŸºäºç±»çš„ä¸­é—´ä»¶

[**åŸºäºç±»çš„ä¸­é—´ä»¶**](https://docs.langchain.com/oss/python/langchain/middleware#class-based-middleware)æ˜¯ä¸€ç§è‡ªå®šä¹‰ä¸­é—´ä»¶ã€‚

```python
from langchain.agents.middleware import AgentMiddleware, AgentState
from langgraph.runtime import Runtime
from langchain.agents import create_agent
from typing import Any

# å®šä¹‰åŸºäºç±»çš„ä¸­é—´ä»¶ (LoggingMiddleware)
class LoggingMiddleware(AgentMiddleware):
    """
    è¿™ä¸ªç±»å®ç°äº†ä¸¤ä¸ªé’©å­ï¼šbefore_model å’Œ after_modelã€‚
    å®ƒè´Ÿè´£åœ¨æ¨¡å‹è°ƒç”¨å‰åæ‰“å°æ—¥å¿—ä¿¡æ¯ã€‚
    """
    
    # é’©å­ 1: åœ¨æ¨¡å‹è°ƒç”¨ä¹‹å‰è§¦å‘
    def before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        print(f"çŠ¶æ€ä¸­åŒ…å« {len(state['messages'])} æ¡æ¶ˆæ¯ã€‚")
        return None

    # é’©å­ 2: åœ¨æ¨¡å‹è°ƒç”¨ä¹‹åè§¦å‘
    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
        last_message_content = state['messages'][-1].content
        print(f"æ¨¡å‹å·²è¿”å›ï¼ˆéƒ¨åˆ†å†…å®¹ï¼‰: {last_message_content[:30]}...")
        return None

# å®ä¾‹åŒ–åŸºäºç±»çš„ä¸­é—´ä»¶
logging_instance = LoggingMiddleware() 

agent = create_agent(
    model="gpt-4o",
    middleware=[logging_instance],
)

# æç¤ºï¼šå®é™…è¿è¡Œæ—¶ï¼Œè¿™ä¸¤ä¸ªé’©å­ä¼šåœ¨ Agent å†…éƒ¨è°ƒç”¨ LLM æ—¶è‡ªåŠ¨æ‰§è¡Œã€‚
print("\nAgent åˆ›å»ºå®Œæˆã€‚")
print("å½“ Agent è¿è¡Œæ—¶ï¼ŒLoggingMiddleware ä¸­çš„ before_model å’Œ after_model å°†è‡ªåŠ¨è¢«è°ƒç”¨ã€‚")
```

# Prompts

Prompts ç”¨äºç®¡ç†æç¤ºè¯æ¨¡æ¿ï¼Œä½¿äº¤äº’æ›´å¯æ§ã€‚

```python
SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

agent = create_agent(
    model=llm,
    # ä¼ å…¥ SYSTEM_PROMPT ç»„ä»¶
    system_prompt=SYSTEM_PROMPT,
    
    # ä¼ å…¥å…¶å®ƒç»„ä»¶
)
```

# Tools

[Tools](https://docs.langchain.com/oss/python/langchain/tools) ç»„ä»¶ä¸º Agent æä¾›å„ç§å·¥å…·ã€‚

```python
from langchain.tools import tool, ToolRuntime

@tool
def get_weather_for_location(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""
```

**åœ¨ä»¥ä¸Šç¤ºä¾‹ä¸­ï¼š**

- ä½¿ç”¨ [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) è£…é¥°å™¨
- `"""Get weather for a given city."""`ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œå‡½æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²ä¼šæˆä¸ºå·¥å…·çš„æè¿°ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£ä½•æ—¶ä½¿ç”¨è¯¥å·¥å…·ã€‚
- åœ¨æç¤ºè¯ä¸­å‘Šè¯‰æ¨¡å‹ï¼Œç”¨è¿™ä¸ªå·¥å…·å¹²ä»€ä¹ˆã€‚

# Streaming

[Streaming](https://docs.langchain.com/oss/python/langchain/streaming)ï¼Œæµå¼ä¼ è¾“ï¼Œç”¨äºæ˜¾ç¤ºå„ä¸ªèŠ‚ç‚¹çš„å®æ—¶æ›´æ–°ï¼Œè€Œä¸æ˜¯å•çº¯æ˜¾ç¤ºæœ€ç»ˆç»“æœã€‚

# Structured Output

[**Structured output**](https://docs.langchain.com/oss/python/langchain/structured-output)ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰å…è®¸ä»£ç†ä»¥ç‰¹å®šä¸”å¯é¢„æµ‹çš„æ ¼å¼è¿”å›æ•°æ®ã€‚

## åŸºæœ¬å®ç°

```python
def create_agent(
    # ...
    response_format: Union[
        ToolStrategy[StructuredResponseT],
        ProviderStrategy[StructuredResponseT],
        type[StructuredResponseT],
    ]
```

# å¤šè½®å¯¹è¯

## åªä½¿ç”¨æ¨¡å‹çš„è„šæœ¬

```bash
pip install langchain
pip install langchain-google-genai
pip install python-dotenv
```

```python
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from typing import List

# ---------------æŒ‡å®šæ¨¡å‹---------------------
# è®¾ç½® API
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# å®ä¾‹åŒ–æ¨¡å‹
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GEMINI_API_KEY
)

# ----------------åˆå§‹åŒ–å¯¹è¯å†å²-------------------------
# ç”¨äºå­˜å‚¨å®Œæ•´çš„å¯¹è¯å†å²
chat_history: List[BaseMessage] = []

# ----------------è¿è¡Œå¤šè½®å¯¹è¯å¾ªç¯-------------------------
def chat_loop():
    """
    è¿è¡Œä¸€ä¸ªå‘½ä»¤è¡Œäº¤äº’å¾ªç¯ï¼Œå®ç°æŒç»­çš„å¤šè½®å¯¹è¯ã€‚
    """
    global chat_history
    
    print("--- ğŸ¤– Gemini å‘½ä»¤è¡ŒèŠå¤©åŠ©æ‰‹å·²å¯åŠ¨ ---")
    print("è¾“å…¥æ‚¨çš„æé—®ã€‚è¾“å…¥ 'é€€å‡º' æˆ– 'q' ç»“æŸä¼šè¯ã€‚")
    
    # å°†ä¸€ä¸ªç³»ç»ŸæŒ‡ä»¤æ·»åŠ åˆ°å†å²è®°å½•çš„å¼€å¤´ï¼Œè®¾ç½® AI çš„è§’è‰²
    system_instruction = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ä¸”ä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚è¯·è®°ä½ç”¨æˆ·çš„ä¸Šä¸‹æ–‡å’Œä¹‹å‰çš„å¯¹è¯ã€‚"
    
    # å¾ªç¯ç­‰å¾…ç”¨æˆ·è¾“å…¥
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ğŸ‘¤ æ‚¨: ").strip()
            
            # æ£€æŸ¥é€€å‡ºå‘½ä»¤
            if user_input.lower() in ["é€€å‡º", "q", "exit", "quit", "exit()"]:
                print("\nğŸ‘‹ è°¢è°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
                
            if not user_input:
                continue

            # 1. åˆ›å»ºä»£è¡¨å½“å‰ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            current_human_message = HumanMessage(content=user_input)
            
            # 2. æ„å»ºå®Œæ•´çš„è¾“å…¥æ¶ˆæ¯åˆ—è¡¨ï¼šå†å²è®°å½• + å½“å‰è¾“å…¥
            full_messages = chat_history + [current_human_message]
            
            # 3. è°ƒç”¨æ¨¡å‹
            # æ³¨æ„ï¼šæ¨¡å‹è°ƒç”¨æ˜¯é˜»å¡çš„ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
            ai_response_message = model.invoke(full_messages)
            
            # è·å– AI çš„æ–‡æœ¬å›å¤
            ai_response_text = ai_response_message.content
            
            # 4. æ‰“å°å›å¤å¹¶æ›´æ–°å¯¹è¯å†å²
            print(f"ğŸ¤– AI: {ai_response_text}")
            
            # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½•
            chat_history.append(current_human_message)
            # å°† AI çš„å›å¤æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
            chat_history.append(ai_response_message)
            
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯: {e}")
            print("ä¼šè¯ç»“æŸã€‚")
            break

# å¯åŠ¨èŠå¤©å¾ªç¯
if __name__ == "__main__":
    chat_loop()
```

## æ¨¡å‹+Flask

è¯¦è§é¡¹ç›®ï¼šBuild a Chatbot in Web Page with Flask & Langchain

<img src="assets/image-20251106230113201.png" alt="image-20251106230113201" style="zoom:50%;" />
